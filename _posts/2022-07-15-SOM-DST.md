---
title: "[Paper Review] Efficient dialogue state tracking by selectively overwriting memory"
last_modified_at: 2022-07-15 00:00:00 -0400
categories: 
  - nlp paper
tags:
  - update
toc: true
use_math: true
toc_label: "Getting Started"
---

# Efficient dialogue state tracking by selectively overwriting memory
> Kim, Sungdong, et al. "Efficient dialogue state tracking by selectively overwriting memory." arXiv preprint arXiv:1911.03906 (2019).

## Abstract

dialogue state tracking (DST)에서 기존 미리 정의된 ontology 기반 접근법의 확장성과 일반화 문제를 해결하기 위해, 최근 연구들은 open-vocabulary-based setting에 초점을 두고 있음

하지만, 최근 연구들은 처음부터 끝까지 모든 turn의 대화 상태(dialogue state)를 예측한다는 점에서 비효율적이라는 문제가 있음

본 논문에서는 dialogue state를 명시적인 고정 크기의 메모리로 간주하고, 효율적인 DST를 위한 선택적 overwriting mechanism을 제안함

- 논문에서 제안한 메커니즘

  1. 각 memory slots에 대한 state operation 예측

  2. 예측된 state operations에 따라 생성이 필요한 slot에 한하여 생성하고, memory를 새로운 value로 overwriting 수행

또한, 논문에서는 DST를 2개의 sub-task로 나누었고, 정확한 역할 분담으로 decoder가 생성에만 집중할 수 있도록 하여 부담을 덜어줌

> 이는 학습의 효율성과 DST의 성능을 향상시킴

- SOM-DST 성능

  - open vocabulary-based DST setting에서 MultiWOZ 2.1에서 53.01% MultiWOZ 2.0에서 51.72%의 joint goal accuracy로 SOTA 달성
  
## Selectively Overwriting Memory for Dialogue State Tracking

<img src="/assets/img/SOM-DST/fig2.JPG" width="100%" height="100%">

**Dialogue State**

논문에서는 대화 turn t에서의 dialogue state를 ${B_{t} = \lbrace (S^j,V^j_t) | 1 \le j \le J \rbrace}$ 로 표기하고, 
slot S가 key이고 그에 해당하는 V가 value인 고정된 사이즈의 메모리로 간주

> 여기에서 J는 slots의 전체 개수

MultiWOZ 20.과 MultiWOZ 2.1의 관행을 따라 논문에서 domain 명과 slot 명의 concatenation을 "slot"으로 사용

**Special Value**

special value인 NULL, DONCARE를 사용

- NULL : turn에서 slot에 대한 정보가 없음을 의미하는 값

  ex) 대화가 시작되기 전에 dialogue $B_{0}$는 모든 slot의 value 값으로 NULL을 가짐
  
- DONCARE : 대화에서 더이상 추적할 필요가 없거나 중요치 않은 값을 나타낼 때 사용

**Operation**

모든 turn t에서 state operation predictor에 의해 연산이 선택됨 

```
operations : CARRYOVER, DELETE, DONCARE, UPDATE
```

현재 turn에 해당하는 모든 slot $S^j$의 value $V_{t}^t$를 설정하기 위해 연산을 수행

<img src="/assets/img/SOM-DST/eq1.JPG" width="60%" height="60%">

> CARRYOVER인 경우에만 이전 value 값을 유지하고, 나머지 연산인 경우에는 변경됨
> 
> DELETE는 이전 value 삭제하고 NULL로 설정
> 
> DONCARE는 현재 value 값을 DONCARE로 설정
> 
> UPDATE는 현재 value 값을 새롭게 생성

**state operation predictor**는 classification task로 state operation prediction을 수행하고, **slot value generator**는 slots의 values를 찾기 위해 slot value 생성을 수행 (UPDATE 일 때만 생성)

-> 이 두가지 구성요소가 현재 turn의 dialogue state를 예측하기 위해 공동으로 학습됨

### State Operation Predictor

**Input Representation**

turn t에서의 dialogue utterances를 ${D_t = A_t \oplus ; \oplus U_t \oplus [SEP]}$로 표기

```
A : system response
U : user utterance 
; : A와 U를 구분해주는 스페셜 토큰
[SEP] : dialogue turn이 끝났음을 표시해주는 스페셜 토큰
```

turn t에서의 dialogue state를 ${B_t^j = [SLOT]^j \oplus S^j \oplus - \oplus V_t^j}$로 표기

```
[SLOT] : BERT의 [CLS] 토큰과 유사하게 j번째 슬롯 정보를 담는 스페셜 토큰
- : slot과 value를 구분해주는 스페셜 토큰
```

> 논문에서는 모든 slot j에 대한 스페셜 토큰으로 [SLOT]를 동일하게 사용

state operation predictor로 pre-trained BERT encoder 사용

input : 이전 turn dialog utterances, 현재 turn dialogue utterances, 이전 turn dialog state의 concatenation

$X_t = [CLS] \oplus D_{t-1} \oplus D_t \oplus B_{t-1}$

> [CLS] 토큰은 모든 turn의 input의 앞에 붙는 스페셜 토큰이고, 입력으로 이전 대화 상태를 사용하는 것은 모델에게 명시적이고 압축된 유익한 정보를 주는 역할


input token $X_t$의 임베딩, segment id의 임베딩, position 임베딩으로 합이 BERT의 입력으로 들어감

> segment embedding은 $D_{t-1}$에 속하는 토큰은 0으로 $D_t, B_{t-1}$에 속하는 토큰은 1로 설정

**Encoder Output**

encoder의 output representation은 시퀀스 정보를 인코딩한 $H_t$, [CLS] 토큰을 인코딩한 $h_t^{[CLS]}$, [SLOT] 토큰을 인코딩한 $h_t^{[SLOT]^j}$ 

<img src="/assets/img/SOM-DST/eq3.JPG" width="50%" height="50%">

> [CLS] 벡터를 feed-forward layer에 통과시켜 전체 시퀀스 정보를 응축한 $h_t^{X}$를 얻을 수 있음

**State Operation Prediction**

인코더 output인 각 slot representation $h_t^{[SLOT]^j}$를 이용하여 four-way classification 수행

<img src="/assets/img/SOM-DST/eq4.JPG" width="50%" height="50%">


---
title:  "Efficient Estimation of Word Representations in Vector Space review"
last_modified_at: 2022-03-11 00:00:00 -0400
categories: 
  - nlp paper
tags:
  - update
toc: true
toc_label: "Getting Started"
---

# Efficient Estimation of Word Representations in Vector Space
> Mikolov, Tomas, et al. "Efficient estimation of word representations in vector space." arXiv preprint arXiv:1301.3781 (2013).

## Abstract

* 매우 큰 데이터셋으로부터 단어들의 연속적인 벡터 representations을 계산하기 위한 2가지 모델 제안

> 모델의 representations 결과를 word similarity task에서 평가하였고, 기존의 neural networks 기반 다른 모델들과 비교함

* 매우 낮은 계산비용으로 정확도에서 큰 향상을 보임

> 16억개의 단어들의 구성된 대규모 데이터셋에서 word vector를 학습시키는데 하루가 걸리지 않음

## Introduction

* 현재 많은 NLP 시스템과 기술들은 단어들을 독립적으로 처리함 (단어간 유사도를 고려하지 않음)

  단어를 독립적으로 처리하는 이유

  > 단순하지만 매우 잘동작함

  > 대규모 데이터로 학습시킨 단순한 모델들이 적은 데이터로 학습시킨 복잡한 시스템보다 성능이 우수함
  
* 하지만, 이런 단순한 방식은 다양한 tasks에서 한계점이 존재함

> automatic speech recognition, machine translation 등 대규모 데이터셋을 구하기 힘든 task에서 한계에 부딪힘
  
* 최근 머신러닝 기술들의 발전하면서 대규모 데이터셋에서 복잡한 모델을 학습시키는 것이 가능해졌고, 이는 일반적으로 단순한 모델의 성능을 뛰어넘음

* 분산 표현(Distributed Representation)을 사용하는 것이 가장 성공적임

> 예를 들어, 신경망 기반 언어 모델은 N-gram 모델을 훨씬 능가함

> 분산 표현이란? 분산 표현은 분포 가설을 이용하여 텍스트를 학습하고, 단어의 의미를 벡터의 여러 차원에 분산하여 표현 

> https://wikidocs.net/22660 

### Goals of the Paper

* 10억개 이상의 단어들과 vocabulary에서 100만개 이상의 단어들을 가진 대규모 데이터로부터 질좋은 단어 벡터들을 학습시키기 위한 기법들을 소개함

> 현재까지는 몇 억 단어 이상의 대규모 데이터셋에서 성공적으로 학습시킨 모델이 제안된 적 없음

* vector representations 결과를 평가하기 위해 가장 최근 제안된 기술들을 사용

> 유사한 단어가 서로 가까이 있을 것이고, 단어들 사이에 여러 수준의 유사성을 가질 수 있다고 기대함

* 단어 representations의 유사성은 단순한 구문 규칙들을 넘어선다는 점이 발견됨

> vector('King') - vector('Man') + vector('Woman') = 'Queen' 

* 단어들 사이에 선형 규칙들을 보존하는 새로운 모델을 개발하여 벡터 연산의 정확도를 높이기 위해 노력함

* 구문 규칙성과 의미적 규칙성을 둘 다 측정하기 위한 새로운 테스트셋 제작하고, 이러한 규칙성이 높은 정확도로 학습될 수 있음을 보임

* 학습시간과 정확도가 단어 벡터의 차원과 학습 데이터의 양에 얼마나 의존적인지에 대해 논의함

### Previous Work

* 연속적인 벡터로 단어들을 표현하는 방식은 오랜 역사를 가짐 (NNLM : neural network language model)

* 가장 유명한 모델은 "A neural probabilistic language model"

> 하나의 linear projection layer와 하나의 non-linear hidden layer를 가진 feedforward neural network가 단어 벡터 representation과 통계적 언어 모델을 공동으로 학습시키기 위해 사용됨

<img src="/assets/img/Word2vec/ref1.JPG" width="70%" height="70%">

* 또 다른 흥미로운 NNLM 모델은 하나의 hidden layer를 가진 neural network를 사용하여 단어 벡터를 학습하고, 학습된 단어 벡터가 NNLM을 학습시키기 위해 사용되는 two-step 모델

> 연구에서 이러한 구조를 확장하였고, 첫번째 step에 초점을 둠

* 단어 벡터가 많은 NLP 응용 프로그램을 개선하고 단순화하는 데 사용될 수 있음이 밝혀지면서, 단어 벡터 추정이 다양한 모델을 사용하여 수행되고 다양한 말뭉치에 대해 학습됨

> 하지만, 이런 구조는 대각 가중치 행렬이 사용되는 log-bilinear 모델의 특정 버전을 제외하면, 훨씬 더 많은 계산 비용이 듦











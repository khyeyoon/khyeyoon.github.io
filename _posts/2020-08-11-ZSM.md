---
title:  "Zooming Slow-Mo: Fast and Accurate One-Stage Space-Time Video Super-Resolution review"
last_modified_at: 2020-08-11 00:00:00 -0400
categories: 
  - Space-Time Video Super-Resolution paper
tags:
  - update
toc: true
toc_label: "Getting Started"
---

# Zooming Slow-Mo: Fast and Accurate One-Stage Space-Time Video Super-Resolution
> Xiang, Xiaoyu, et al. "Zooming Slow-Mo: Fast and Accurate One-Stage Space-Time Video Super-Resolution." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.

## Abstract

* Space-Time Video Super-Resolution : low frame rate(LFR)인 low resolution(LR) video에서 high-resolution(HR) slow-motion video를 생성

* STVSR의 단순한 방식 : video-frame interpolation(VFI) task와 video super-resolution(VSR) task 2가지로 나누어서 해결

> 이런 two-stage 방식들은 두 task 사이 연관성을 이용할 수 없음

> VFI나 VSR networks는 large frame-synthesis module 또는 large reconstruction module을 가짐 (large model size, time-consuming)

* One-stage space-time video super-resolution framework 제안

> LFR, LR video에서 직접 HR slow-motion video를 만들어줌

1) feature temporal interpolation network

존재하지 않는 LR video frame을 VFI network처럼 합성하지 않고, 먼저 local temporal contexts를 포착하면서 존재하지 않는 LR video frame에서의 LR frame features를 interpolation 

2) deformable ConvLSTM

global temporal contexts를 더 잘 얻기 위해, temporal 정보의 alignment와 aggregation을 동시에 수행

3) deep reconstruction network

최종적으로, HR slow-motion video frames을 예측

* 광범위한 실험을 진행하여 SOTA를 달성하였고 기존의 방식(two-stage 방식)보다 3배 빠른 속도

<img src="/assets/img/ZSM/fig1.PNG" width="100%" height="100%" title="70px" alt="memoryblock">

## Introduction 

* Space-Time Video Super-Resolution (STVSR) : low resolution과 low frame rate를 가진 input video로부터 high space-time resolution을 가진 사실적인 video sequence를 생성하는 것

> film making, high-definition television과 같은 응용분야에서 HR slow-motion videos를 필요로 함

* * *

* STVSR을 다루는 기존 방식들은 일반적으로 hand-crafted regularization과 강한 assumptions을 함

> 문제점 : 강한 constraints으로 인해, model이 다양하고 방대한 space-time visual patterns을 만들어내는 것을 제한시키며 계산비용도 많이 듦

* * *

* deep convolutional neural networks가 다양한 video restoration tasks에서 좋은 결과를 보임

> video restoration task : video frame interpolation, video super-resolution, video deblurring

* STVSR network를 video frame interpolation method와 video super-resolution method를 직접 결합시켜 디자인함 (two-stage)

> 먼저 VFI를 통해 존재하지 않는 LR frames을 생성하고, 그 이후에 VSR을 사용하여 LR video frames을 HR frames으로 reconstruction 수행

> 문제점 : two-stage 방식은 temporal interpolation과 spatial super-resolution를 개별적으로 처리하면서 두 task 사이 상관관계를 활용할 수 없고, 
두 network 모두 규모가 커서 파라미터의 수가 많고 계산 비용이 많이 듦    

* * *

* 통합된 one-stage STVSR framework 제안 : temporal interpolation과 spatial super-resolution을 동시에 학습

* deformable feature interpolatoin function : temporally interpolating intermediate(input video에서 존재하지 않은 frame) LR frame features

> learnable offsets이 도움이 되는 local temporal contexts를 융합시키고, temporal interpolation이 복잡한 visual motions을 잘 다룰 수 있도록 도움

* 새로운 deformable ConvLSTM model 도입 : temporal alignment, aggregation과 함께 global contexts를 효과적으로 얻을 수 있게 함

* deep SR reconstruction network : aggregated LR features를 input으로 받아서 HR frame video로 reconstruction 

* 훨씬 적은 parameters로 SOTA 성능을 냄

* * * 

* The contributions

1) One-stage space-time super-resolution network 제안

통합된 framework로 temporal interpolation과 spatial SR을 동시에 수행하면서 상호연관성을 이용할 수 있고, 계산 효율이 좋음 (reconstruction network가 하나만 필요)

2) frame feature temporal interpolation network, deformable ConvLSTM

frame feature temporal interpolation network : 존재하지 않는(intermediate) LR frames을 위한 deformable sampling을 기반으로 local temporal contexts를 얻음

deformable ConvLSTM : 명시적으로 temporal alignment capacity를 향상시키고, 큰 motions을 잘 다루기 위한 global temporal contexts를 이용

3) state-of-the-art 달성

Vid4와 Vimeo datasets에서 SOTA를 달성했고, two-stage network에 비해 3배 빠른 속도 (model size는 4배정도 작음)

## Space-Time Video Super-Resolution

* 빠르고 정확하게 space와 time domains 모두에서 resolution을 증가시키기 위한, one-stage space-time super-resolution framework 제안

<img src="/assets/img/ZSM/fig2.PNG" width="100%" height="100%" title="70px" alt="memoryblock">

input : LR, LFR video sequence (low-resolution이고, 낮은 rate를 갖는 video sequence)

1) feature extractor

하나의 convolution layer와 k1 residual blocks으로 구성됨

2) frame feature interpolation module

intermediate frames의 LR feature maps을 만들어 냄

3) deformable ConvLSTM 

연속적인 feature maps을 처리하기 위해 사용하며, temporal alignment와 aggregation을 동시에 수행할 수 있음

> temporal 정보를 더 잘 이용할 수 있음

4) HR frame reconstructor

최종적으로, aggregated feature maps으로부터 HR slow-mo video sequence 복원

* * *

### 3.1 Frame Feature Temporal Interpolation

<img src="/assets/img/ZSM/fig3.PNG" width="60%" height="60%" title="70px" alt="memoryblock">

직접적으로 intermediate feature map을 만들어내기 위한 feature temporal interpolation function 학습을 제안

<img src="/assets/img/ZSM/eq1.PNG" width="60%" height="60%" title="70px" alt="memoryblock">

> T : sampling function, Φ : sampling parameter, H : blending function

F_1과 F_3 사이의 motion 정보를 이용하여 대략적인 forward, backward motion 정보로 사용

deformable sampling functions : frame feature temporal interpolation을 위한 motion 정보 capturing (implicitly)

> 풍부한 local temporal contexts를 이용하면서, 매우 큰 motions도 다룰 수 있도록 함

2개의 sampling functions은 동일한 network design을 공유하지만, 다른 weights 사용

* learnable offset (=sampling parameter)

<img src="/assets/img/ZSM/eq2.PNG" width="60%" height="60%" title="70px" alt="memoryblock">

> implicitly learn to capture the forward motion information

* sampling function

<img src="/assets/img/ZSM/eq3.PNG" width="60%" height="60%" title="70px" alt="memoryblock">

> deformable convolution 수행

* simple linear blending function H

<img src="/assets/img/ZSM/eq4.PNG" width="60%" height="60%" title="70px" alt="memoryblock">

> F는 intermediate HR frame I 를 예측하는데 사용됨

### 3.2 Deformable ConvLSTM

<img src="/assets/img/ZSM/fig4.PNG" width="60%" height="60%" title="70px" alt="memoryblock">

* 인접 frames에서 temporal contexts를 aggregation 하기 위해 ConvLSTM을 사용

<img src="/assets/img/ZSM/eq5.PNG" width="60%" height="60%" title="70px" alt="memoryblock">

> ConvLSTM은 현재 input feature map과 이전 state 사이의 motions을 capturing 하기 때문에 large motions을 다루는 능력이 제한되어 있음

* Deformable ConvLSTM

<img src="/assets/img/ZSM/eq6.PNG" width="60%" height="60%" title="70px" alt="memoryblock">

> large motions을 더 잘 다룰 수 있고, temporal 정보를 완전하게 이용할 수 있음 (global temporal contexts)

### 3.3 Frame Reconstruction

input : hidden state h_t

output : input에 해당하는 HR frame

* deep features를 학습하기 위해 k2개의 residual blocks을 사용하고, sub-pixel upscaling module(PixelShuffle)을 이용하여 HR frame reconstruction

* Reconstruction loss function

<img src="/assets/img/ZSM/eq7.PNG" width="60%" height="60%" title="70px" alt="memoryblock">

> Charbonnier penalty function를 loss term으로 사용

















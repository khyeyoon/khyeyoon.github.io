---
title: "Enriching word vectors with subword information review"
last_modified_at: 2022-03-18 00:00:00 -0400
categories: 
  - nlp paper
tags:
  - update
toc: true
toc_label: "Getting Started"
---

# Enriching word vectors with subword information
> Bojanowski, Piotr, et al. "Enriching word vectors with subword information." Transactions of the association for computational linguistics 5 (2017): 135-146.

## Abstract

라벨링 되지 않은 대규모 데이터로 학습된 연속 단어 표현은 많은 자연어처리 task에서 유용함

  > 이런 벡터 표현을 학습하는 유명한 모델들은 각 단어에 고유한 벡터를 할당함으로써 단어들의 형태를 무시하는 문제점이 있음

  > 이러한 문제점은 대규모 단어들과 많은 희귀한 단어들이 포함된 언어에서 도드라짐

* Skip-gram model을 기반으로 한 새로운 접근법 제안

  > 각 단어들은 character n-grams의 집합으로 표현됨

  > 하나의 벡터는 각 character n-gram과 연관이 있고, 단어는 이러한 representations의 합으로 표현됨

* 논문에서 제안한 접근법은 빠르고, 학습 데이터에서 등장하지 않은 단어에 대해서도 word representation 수행이 가능함

9개 언어에서 word similarity, analogy tasks에서 word representations을 평가

  > 최근 제안된 morphological word representations 방식들과 비교하였을 때, SOTA 성능을 보임

## Introduction

자연어처리 분야에서 연속 벡터 표현을 학습시키는 것은 오랜 역사를 가지고 있음

> 이러한 표현은 일반적으로 동시 발생 통계를 사용하여 라벨링 되지 않은 대규모 데이터셋에서 파생됨

distributional semantics이라고 알려진 연구 분야에서 이러한 방법의 특성을 연구함

```
분포 의미론(Distributional Semantics)은 언어 데이터의 큰 샘플에서 분포 속성을 기반으로 언어 항목 간의 의미론적 유사성을 정량화하고 분류하기 위한 이론과 방법을 개발하고 연구하는 연구 분야
분포 의미론의 기본 아이디어는 소위 분포 가설로 요약될 수 있습니다. 
유사한 분포를 가진 언어 항목은 유사한 의미를 갖습니다.
```
  > https://en.wikipedia.org/wiki/Distributional_semantics
   
Collobert와 Weston은 왼쪽 단어 2개, 오른쪽 단어 2개를 통해 하나의 단어를 예측하는 feed-forward neural network를 사용하여 word embeddings을 학습하는 방식을 제안

가장 최근에는 Mikolov가 대규모 데이터셋에서 연속 word representations을 효율적으로 학습하기 위한 단순한 log-bilinear models 제안 (Word2Vec)

* * *

이러한 기법들의 대부분은 vocabulary 속 각 단어를 파라미터 공유없이 하나의 특정 벡터로 나타냄

이런 방법들은 단어들의 내부 구조를 무시하고, 이는 Turkish, Finnish와 같이 형태론적으로 풍부한 언어에서 정확한 표현을 어렵게 함

  > 프랑스어나 스페인어에서 대부분의 동사는 40가지 이상의 다른 굴절 형태를 가지만, 핀란드어에서는 명사에 대해 15가지 경우가 있음

  > 이런 언어들은 학습 데이터에서 전혀 등장하지 않거나 드물게 등장하는 단어 형태를 많이 포함하고 있고, 이는 word representations 학습을 어렵게 만듦

많은 단어 구성이 규칙을 따르기 때문에, 문자 수준 정보를 사용하면 형태학적으로 풍부한 언어에 대한 벡터 표현을 개선할 수 있음

* * *

* character n-grams에 대해 벡터 표현을 학습하고, n-gram 벡터들의 합으로 각 단어를 표현하는 방식을 제안

* 연속적인 skip-gram model을 확장시킨 방식이고, subword 정보를 고려할 수 있도록 함

  > 해당 모델의 강점을 증명하기 위해, 다른 형태적 요소가 존재하는 9개의 언어에서 성능을 평가함

## Related work



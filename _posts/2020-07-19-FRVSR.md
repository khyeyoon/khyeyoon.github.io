---
title:  "[Paper Review] Frame-recurrent video super-resolution"
last_modified_at: 2020-07-19 00:00:00 -0400
categories: 
  - Video Super-Resolution paper
tags:
  - update
toc: true
toc_label: "Getting Started"
---

# Frame-recurrent video super-resolution
> Sajjadi, Mehdi SM, Raviteja Vemulapalli, and Matthew Brown. "Frame-recurrent video super-resolution." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.

**Abstract**

* 기존 방식 : 하나의 HR frame을 생성하기 위해 LR frame batch(여러개의 LR frame)를 처리하고, 전체 video에서 sliding window 방식으로 진행

> 다수의 개별적인 multi-frame super-resolution tasks로 진행함

* 기존 방식 문제점

1) 모든 input frame이 여러번 처리되고 warping 되면서 계산 비용 증가

2) 모든 output frame이 독립적으로 예측되면서 temporally consistent results(시간적으로 일관성이 있는 것)를 얻는 것이 제한됨

* end-to-end trainable frame-recurrent video super-resolution framework(FRVSR) 제안

> 이전에 예측된 frame(HR)을  다음 frame 예측에 사용

* FRVSR의 강점

1) temporally consistent results

2) 모든 step에서 하나의 image를 warping 하면서 계산 비용이 감소

3) recurrent 한 특성으로 추가적인 계산 비용없이 이전 frames에 동화될 수 있음 (이전 frames에서 많은 부분을 가져올 수 있음)

* 광범위한 evaluations과 이전 방식들과 비교를 통해 FRVSR의 성능을 입증

* SOTA 달성

**Introduction**

* SISR에서는 오직 spatial statics로부터 high-frequency details을 복원하는 것이 중요하지만, Video SR에서는 복원 성능을 높이기 위해서 temporal relationships을 이용하는 것이 중요함

* Video SR에서는 가능한 많은 LR frames으로부터 정보를 결합하는 것이 좋은 성능을 냄

* * *

* 최근 Video SR 방식은 하나의 HR frame을 생성하기 위해 여러개의 LR frame을 결합(a large number of separate multi-frame super-resolution subtasks)

> 모든 input frame은 여러번 처리되기 때문에 계산 비용이 높음

> 각 output frame이 독립적으로 생성되면서 시간정보를 활용하기 어려움 (resulting in unpleasing flickering artifacts)

* * *

* end-to-end trainable frame-recurrent video super-resolution(FRVSR) framework 제안

* 모든 video frame을 독립적으로 예측하지 않고, 이전 예측 frame(HR)을 다음 frame 예측의 input으로 넣어줌 (recurrent approach)

> 모든 input frame은 한번만 처리되어 계산 비용 감소

> 지난 frames 정보가 나중 frames으로 전달됨 (HR estimation 전달)

> 이전 예측 frames을 직접 다음 step에 넘겨주는 것은 model이 fine details을 재생성하고, temporally consistent videos를 만드는 것에 도움을 줌

* * *

* 제안된 framework의 성능을 분석하기 위해, single image와 video super-resolution baselines을 통해 비교

* 광범위한 실험은 FRVSR의 성능이 recurrent steps의 수, network size, noise의 양에 따라 얼마나 변하는지를 보여줌

* 제안된 방법은 다양한 setting에서 baselines의 성능을 능가 (quality, efficiency)

* FRVSR과 기존 video SR 방식을 비교하여 SOTA임을 보여줌

* * *

* Contributions

1) recurrent framework 제안 : 이전 예측 frame을 다음 frame 예측에 사용

> temporally consistent results

2) 기존의 방식과 다르게 계산 비용의 증가없이 넓은 temporal range(??)에 대해 정보를 전달할 수 있음

3) end-to-end trainable, pre-training stages가 필요하지 않음

4) 광범위한 실험 수행 : 제안된 framework와 관련된 baselines을 다양한 환경에서 분석

5) 제안된 framework는 기존 SOTA를 상당히 능가

* * *

**Method**

<img src="/assets/img/FRVSR/fig2.PNG" width="100%" height="100%" title="70px" alt="memoryblock">

* 3.1 FRVSR Framework

> trainable components(red) : FNet(optical flow estimation network), SRNet(super-resolution network)

1) Flow estimation

FNet은 현재 LR frame과 이전 LR frame 사이의 flow를 예측함 (normalized low-resolution flow map을 생성)

<img src="/assets/img/FRVSR/eq1.PNG" width="60%" height="60%" title="70px" alt="memoryblock">

2) Upscaling flow

scale factor s로 bilinear interpolation을 수행하여 upscaling

<img src="/assets/img/FRVSR/eq2.PNG" width="60%" height="60%" title="70px" alt="memoryblock">

3) Warping previous output 

HR flow map을 이용하여 이전 예측 frame을 현재 frame으로 warping

<img src="/assets/img/FRVSR/eq3.PNG" width="60%" height="60%" title="70px" alt="memoryblock">

4) Mapping to LR space

warped 이전 output(HR space)을 LR space로 mapping (space-to-depth transformation)

<img src="/assets/img/FRVSR/eq4.PNG" width="60%" height="60%" title="70px" alt="memoryblock">

> space-to-depth transformation

<img src="/assets/img/FRVSR/fig3.PNG" width="60%" height="60%" title="70px" alt="memoryblock">

5) Super-Resolution

LR space로 mapping 시킨 이전 예측 frame을 현재 LR frame과 concatenation (SRNet의 input으로 들어감)

<img src="/assets/img/FRVSR/eq6.PNG" width="60%" height="60%" title="70px" alt="memoryblock">










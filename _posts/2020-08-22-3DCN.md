---
title:  "Video inpainting by jointly learning temporal structure and spatial details review"
last_modified_at: 2020-08-22 00:00:00 -0400
categories: 
  - Video inpainting paper
  - "2019"
tags:
  - update
toc: true
toc_label: "Getting Started"
---

# Video inpainting by jointly learning temporal structure and spatial details
> Wang, Chuan, et al. "Video inpainting by jointly learning temporal structure and spatial details." Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. 2019.

## Abstract

* video frames의 잃어버린 부분을 복원하기 위한 새로운 data-driven video inpainting method 제시

* 새로운 deep learning 구조로 2가지 sub-networks로 이루어짐

1. The temporal structure inference network 

3D fully convolutional 구조로 이루어져 있고, 높은 계산 비용을 감안하여 low-resolution video volume을 완성하는 것을 학습함

2. The spatial detail recovering network

low-resolution 결과를 통해 temporal guidance를 제공받고, 2D fully convolutional network를 사용하여 image 기반 inpainting을 수행

> original resolution으로 복원된 video frames을 만들어냄

* 2-step network 구조는 각 frame의 spatial quality과 frame에 걸친 temporal coherence를 모두 보장

> 2개의 sub-networks를 함께 end-to-end 방식으로 학습을 진행

* 3개의 datasets을 통해 양적, 질적 평가를 진행하였고, 이전의 learning based methods의 성능을 능가

## Introduction

* holes이 존재하는 image나 video가 주어지면, inpainting 방식들은 보기에 자연스러운 결과를 생성하기 위해 잃어버린 video content를 복원하기 위해 노력함

> object removal에 의해 생성된 holes

* inpainting 기술에 요구되는 2가지 조건

1) 잃어버린 부분에 생성된 content는 주변 content와 의미론적으로 정확해야 함 (semantically correct)

2) 원래 holes을 알아볼 수 없도록 매끄럽게 채워야 함

* * *

* 본 논문은 image inpainting에 temporal dimension을 추가한 video inpainting에 초점을 맞춤

> 1) 잃어버린 video content를 복원시키는 것은 각 frame의 spatial context 뿐만 아니라 frames 간 motion context도 필요로 함

> 2) output video는 high spatio-temporal consistency를 유지해야 함 (global context-level, local image-feature-level)

* 2D를 3D로 확장하기 위한 많은 시도가 있었으나, 이는 challenging하고 한계점이 존재

* * *

* 새로운 end-to-end deep learning 구조를 제시

<img src="/assets/img/3DCN/fig1.PNG" width="80%" height="80%" title="70px" alt="memoryblock">

* network는 temporal structure prediction sub-network와 spatial detail recovering sub-network로 구성

1) Temporal structure prediction sub-network

3D volume으로 video를 처리하고, input으로 downsampled video를 취함

Encoder-Decoder 구조로 3D CNN을 이용하여 holes을 채움

output volume을 temporal structure guidance로 사용 (spatial details은 부족하지만 motion 정보를 담고 있기 때문)

2) Spatial detail recovering sub-network

input : original video, temporal structure guidance 

완성된 비디오 프레임을 원래 resolution로 생성함

2D Encoder-Decoder 구조















---
title: "[Paper Review] Language models are unsupervised multitask learners"
last_modified_at: 2022-08-08 00:00:00 -0400
categories: 
  - nlp paper
tags:
  - update
toc: true
use_math: true
toc_label: "Getting Started"
---

# Language models are unsupervised multitask learners
> Radford, Alec, et al. "Language models are unsupervised multitask learners." OpenAI blog 1.8 (2019): 9.

# Abstract

질의 응답(AQ), 기계 번역, reading comprehension, 요약과 같은 자연어 처리 task는 task에 맞는 데이터셋으로 지도 학습하는 접근법이 일반적입니다.

본 논문에서는 task에 대한 명시적인 지도없이 새로운 데이터셋인 **WebText**를 통한 학습만으로 다양한 task를 처리할 수 있는 언어 모델을 제시합니다.

- Question Answering
  
  문서와 질문이 함께 입력으로 들어오면, 언어 모델이 정답을 생성하는 방식
  
  CoQA 데이터셋 기준 55 F1 Score를 달성하였고, 이는 127,000 이상의 학습 데이터를 사용하지 않고 3~4개의 베이스라인 성능을 따라잡았습니다.
  
언어 모델의 용량은 zero-shot task의 성공을 위해 필수적이고, 이를 증가시키면 전반적인 tasks의 성능을 로그 선형에 비례하여 향상시킵니다.

**GPT-2**는 1.5B 파라미터를 갖는 트랜스포머로 논문에서 제안한 모델 중 가장 큰 모델이고, 8개 중 7개의 언어 모델링 데이터셋에서 zero-shot 환경에서 SOTA를 달성하였습니다.

하지만, GPT-2는 **WebText**에서 underfitting 현상을 보입니다.

<!-- 모델의 샘플은 이러한 개선 사항을 반영하고 일관된 텍스트 단락을 포함합니다. 

이러한 발견은 자연적으로 발생하는 데모에서 작업을 수행하는 방법을 배우는 언어 처리 시스템을 구축하기 위한 유망한 경로를 제안합니다.  -->

<img src="/assets/img/GPT-2/fig1.JPG" width="100%" height="100%">

# Introduction

머신러닝 시스템은 대규모 데이터셋, 고용량 모델, 지도 학습의 결합으로 빠르게 성장해왔습니다.

하지만, 여전히 이러한 시스템은 데이터 분포의 작은 변화에도 매우 민감하고, task 제한적인 특성을 갖습니다.

> 현재 시스템은 전반적으로 잘하는 것 보다는 좁은 분야를 전문적으로 한다고 특징지을 수 있습니다.

본 논문에서는 다양한 tasks를 수행할 수 있는 더 일반적인 시스템을 만들고자 합니다.

> 각 task를 위해 학습 데이터셋을 구축하거나 라벨링할 필요없이 범용적으로 적용가능한 모델

- 일반적인 머신러닝 접근법 

  1. 원하는 task를 위한 데이터셋 수집

  2. 수집한 데이터셋으로 모델을 학습

  3. 학습 데이터와 동일한 분포의 평가 데이터셋에서 모델을 평가

  이러한 일반적인 접근방식은 특정 task만을 수행할 수 있고, 좁은 활용 범위를 가집니다.
  
  그렇기 때문에 다양한 종류의 입력을 받는 captioning model, reading comprehension system, image classifier의 불규칙한 동작을 다루기에는 적합하지 않습니다.
  
* * *
  
본 논문에서는 단일 도메인 데이터셋으로 단일 task를 학습시키는 방식이 일반화 성능 부족의 원인이라고 추측하였습니다.

현재 구조를 기반으로 강력한 시스템을 만들기 위해서는 다양한 도메인과 task에 대해 학습하고, 성능을 평가해야 합니다.

최근, 이를 위한 GLUE, decaNLP와 같은 여러 벤치마크들이 제안되었습니다.

* * *

**일반화 성능을 높이기 위한 시도**

- Multitask learning 

  일반적인 성능을 향상시키기 위해 등장한 유망한 프레임워크지만, 아직 연구 초기 단계입니다.

  multitask learning을 위해서는 단일 task 보다 훨씬 많은 데이터셋을 필요로 하고, 현재 기술을 따라 잡을 정도로 데이터셋을 생성하고 그에 맞는 objectives를 고안하는 것은 어렵습니다.
  
* * *

[1] 현재 언어 tasks에서 가장 좋은 성능을 내는 시스템은 pre-training과 supervised fine-tuning을 결합한 방식입니다. 

- transfer learning 역사

  1. 학습된 단어 벡터를 task-specific 구조의 입력으로 활용

  2. recurrent networks의 contextual representations 전이

  3. 최근에는 task-specific 구조가 필요하지 않고, 많은 self-attention blocks을 쌓은 구조(transformer)의 전이를 활용

이러한 방법론은 여전히 특정 task를 위한 지도 학습을 필요로 합니다.

[2] 다른 한편, 이용 가능한 데이터셋이 없을 때 특정 tasks(commonsense reasoning, sentiment analysis)를 수행하기 위한 연구들도 수행되었습니다.

본 논문에서는 [1],[2] 연구 방향을 연결하고, 더욱 일반적인 전이 방법을 제안합니다.

- 언어 모델이 추가적인 파라미터나 구조 변경 없이 zero-shot 환경에서 down-stream tasks를 수행할 수 있다는 것을 보여줍니다.

- zero-shot 환경에서 다양한 tasks를 수행하면서 언어 모델의 능력을 강조하고, 앞으로의 가능성을 보여줍니다.

# Approach

## Training Dataset

## Input Representation

## Model

<img src="/assets/img/GPT-2/T2.JPG" width="70%" height="70%">

# Experiments

<img src="/assets/img/GPT-2/T3.JPG" width="100%" height="100%">








  
  
  
  





  





  
  
  
  





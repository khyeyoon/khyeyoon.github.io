---
title:  "Edvr: Video restoration with enhanced deformable convolutional networks review"
last_modified_at: 2020-07-25 00:00:00 -0400
categories: 
  - Video Super-Resolution paper
tags:
  - update
toc: true
toc_label: "Getting Started"
---


# Edvr: Video restoration with enhanced deformable convolutional networks
> Wang, Xintao, et al. "Edvr: Video restoration with enhanced deformable convolutional networks." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. 2019.

**Abstract**

* NTIRE19 challenge에서 공개된 새로운 benchmark REDS

> 1) large motions이 주어진 muliple frames을 어떻게 alignment 하는 방법

> 2) 다양한 motion과 blur를 가진 frames을 효과적으로 융합시키는 방법

* 새로운 Video Restoration framework인 Enhanced Deformable convolutions(EDVR)을 제안

> 위의 challenges(REDS)를 잘 다루는 framework

1) large motions을 잘 다루기 위한 Pyramid, Cascading and Deformable alignment(PCD) module 고안

> deformable convolutions을 사용하여 feature level에서 frame alginment 수행 (coarse-to-fine manner)

2) Temporal and Spatial Attention(TSA) module 제안 

> temporally and spatially 모두 attention이 적용되면서 다음 restoration에서 중요한 features를 강조함

* The NTIRE19 video restoration and enhancement challenges에서 4개의 tracks 모두 1위 차지

* video super-resolution과 video deblurring에서 SOTA 달성

<img src="/assets/img/EDVR/fig1.PNG" width="50%" height="50%" title="70px" alt="memoryblock">

**Introduction**

* REDS dataset : 기존에 존재하던 datasets과 비교했을 때, 크고 복잡한 motions을 포함 (더 현실적이고 challenging)

* 초기 video SR은 image SR의 단순한 확장으로 다루어짐

> 문제점 : 이웃 frames 사이의 temporal 정보를 제대로 활용할 수 없음

* 최근 VSR 연구는 위의 문제점을 해결하기 위해 더 정교한 pipelines 사용 

> feature extraction -> alignment -> fusion -> reconstruction

* video가 occlusion, large motion, severe blurring을 포함하고 있을 때, alignment와 fusion module이 중요한 역할을 함

> 1) align and establish accurate correspondences among multiple frames

> 2) effectively fuse the aligned features for reconstruction

* * *

* Alignment

1) Optical flow : 기존의 많은 방식들은 optical flow를 사용하여 explicit alignment를 수행 

2) dynamic filtering or deformable convolution : dynamic filtering or deformable convolution을 사용하여 implicit motion compensation 수행

> dynamic filtering

<img src="/assets/img/EDVR/dynamic_filter.PNG" width="50%" height="50%" title="70px" alt="memoryblock"> 

> TDAN

<img src="/assets/img/EDVR/TDAN.PNG" width="80%" height="80%" title="70px" alt="memoryblock">

REDS는 기존의 alignmnet algorithms으로 다루기 challengeing한 dataset

> large motions의 경우, single scale에서 motion compensation을 수행하는 것은 어려움 (explicitly or implicitly)

* * *

* Fusion

기존의 fusion 방식은 모든 frames에서 convolution을 사용하여 early fusion을 사용하거나, recurrent networks를 사용하여 점진적으로 multiple frames을 융합시킴

> 각 frame들이 얼마나 중요한 정보를 담고 있는지를 고려하지 않음 (모든 frames은 이미지 복원에 도움이 되는 정보를 각자 다르게 가지고 있음)

* * *

* Our Solution : EDVR

1) an alignment module known as Pyramid, Cascading and Deformable convolutions (PCD)

feature level에서 reference frame과 인접 frame 사이 alignment를 맞추기 위해 deformable convolutions을 사용 (TDAN에서 영감을 얻음)

> large and complex motions을 다루기 위해, coarse-to-fine 방식으로 alignment를 수행한다는 점이 TDAN과 다름

Pyramid 구조 사용

> first align feature in lower scales : coarse -> aligned features to higher scales : fine (정확한 motion compensation을 가능하게 함)

pyramidal alignmnet 연산을 수행한 후에, 추가적인 deformable convolution을 계단식으로 배치하여 alignmnet의 견고성을 높임

2) a fusion module known as Temporal and Spatial Attention (TSA)

multiple aligned features 사이 정보를 결합시키는 것을 돕는 module

> Temporal attention : 각 frame의 visual informativeness를 더 잘 고려하기 위해, reference frame features와 모든 인접 frame features 사이의 element-wise correlation을 계산

> > Correlation coefficients : 각 위치에서의 각 인접 feature의 weight를 측정하여, 이미지를 복원하는데 얼마나 정보력을 갖고 있는지를 나타냄

> Spatial attention : temporal attention fusion을 한 후, 더 효과적으로 cross-channel과 spatial 정보를 이용하기 위해 각 채널의 각 위치에 가중치를 할당하기 위해 spatial attentaion을 적용

* * *

**Methodology**

* 3.1 Overview

<img src="/assets/img/EDVR/fig2.PNG" width="100%" height="100%" title="70px" alt="memoryblock">

input : 2N+1 low-resolution frames -> output : high-resolution reference frame

1) 각 인접 frames은 PCD alignment module에 의해 feature level에서 reference frame에 alignmnent 됨

2) TSA module에서 다른 frames의 image 정보가 융합됨

3) 융합된 features는 reconstruction module로 들어감 (cascade of residual blocks)

> 다른 SISR module로 쉽게 대체 가능함

4) network의 끝 단에서 spatial size를 키우기 위해 upsampling 연산 수행

5) 최종적으로, 예측된 image residual을 upsampled image에 더하여 high-resolution frame을 얻음

Video deblurring과 같은 high-resolution inputs을 갖는 task에도 적용하기 위해, input frames을 처음에 strided convolution으로 down-sampling 시킴

> 대부분의 연산이 low-resolution에서 수행되기 때문에 계산 비용이 많이 절약됨

A PreDeblur module : alignment module을 수행하기 전, blurry inputs을 pre-processing 하고 alignmnet 정확도를 향상시키기 위해 사용

single EDVR model은 SOTA 성능을 달성했음에도 불구하고, NTIRE19 competition에서 성능을 더 끌어올리기 위해 two-stage strategy를 채택함

> Cascaded network로 심각한 motion blur를 제거함 (preceding model에서는 다룰 수 없음)

* 3.2 Alignment with Pyramid, Cascading and Deformable Convolution

<img src="/assets/img/EDVR/fig3.PNG" width="70%" height="70%" title="70px" alt="memoryblock">

alignment를 위해 the modulated deformable module 사용

> optical-flow based methods와 다르게 각 frame의 features에 적용됨

<img src="/assets/img/EDVR/eq1.PNG" width="50%" height="50%" title="70px" alt="memoryblock">

<img src="/assets/img/EDVR/eq2.PNG" width="50%" height="50%" title="70px" alt="memoryblock">

alignment에서 complex motions과 large parallax 문제를 해결하기 위해, PCD module 제안 (based on well established principles in optical flow??)

> pyramidal processing and cascading refinement

pyramid level 별로 stride가 2인 convolution으로 down-sampling 수행

l-th level에서의 offsets과 alignment features가 2배 upsampling(bilinear interpolation)한 l+1-th level의 offsets과 aligned features와 함께 예측됨

<img src="/assets/img/EDVR/eq3.PNG" width="50%" height="50%" title="70px" alt="memoryblock">

* 3.3 Fusion with Temporal and Spatial Attention

<img src="/assets/img/EDVR/fig4.PNG" width="70%" height="70%" title="70px" alt="memoryblock">

fusion에서 frame 간 temporal 관계와 frame 내 spatial 관계가 중요한 이유

> 1) 다른 인접 frames은 동일하게 informative하지 않음 (occlusion, blurry regions and parallax problems)

> 2) preceding alignment stage에서 발생하는 misalignment와 unalignment는 다음 reconstruction 성능에 부정적인 영향을 끼침

=> 효과적이고 효율적인 fusion을 위해 pixel-level에서 인접 frames을 동적으로 합치는 것이 필수적임

TSA fusion module : 각 frame에서 pixel-level로 weight를 합치는 module

> Temporal attention : embedding space에서 frame 사이 유사도를 계산

> > reference frame과 비슷할수록 더 강조시킴

<img src="/assets/img/EDVR/eq5.PNG" width="70%" height="70%" title="70px" alt="memoryblock">

> Spatial attention : 융합된 features에서 계산됨

> > receptive field를 늘리기 위해 pyramid 구조 사용

* 3.4 Two-Stage Restoration

1) preceding model에서 다룰 수 없는 심각한 motion blur를 효율적으로 제거함

2) output frames 사이의 inconsistency를 완화시킴










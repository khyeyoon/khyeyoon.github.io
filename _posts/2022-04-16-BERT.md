---
title: "Bert: Pre-training of deep bidirectional transformers for language understanding review"
last_modified_at: 2022-04-16 00:00:00 -0400
categories: 
  - nlp paper
tags:
  - update
toc: true
use_math: true
toc_label: "Getting Started"
---

# Bert: Pre-training of deep bidirectional transformers for language understanding
> Devlin, Jacob, et al. "Bert: Pre-training of deep bidirectional transformers for language understanding." arXiv preprint arXiv:1810.04805 (2018).

## Abstract

새로운 language representation model인 BERT(Bidirectional Encoder Representations from Transformers) 공개

최근 language representation model과 달리, BERT는 모든 layer에서 양방향 문맥을 고려하여 pre-train 되도록 구성됨

pre-trained BERT는 task를 위한 큰 구조 변형없이 단지 하나의 추가적인 layer를 이용하여 question answering, language inference와 같은 다양한 task에서 SOTA 달성

BERT는 단순하지만 강력하게 동작함

  > 11개의 자연어처리 task에서 SOTA 달성
  >
  > GLUE score 80.5% / MultiNLI 86.7% / SQuAD v1.1 question asnwering Test F1 93.2 / SQuAD v2.0 Test F1 83.1 

## Introduction

언어 모델 pre-training은 많은 자연어처리 task에서 성능향상을 가져옴

* sentence-level task

  natural language inference, paraphrasing와 같이 전체적으로 분석하여 문장 간 관계를 예측하는 task
  
* token-level task

  named entity recognition(NER), question answering과 같이 단어 수준에서 세부적인 예측을 하는 task
  
* * *
  
* pre-trained language representations을 세부적인 task로 적용시키는 방식 

1) feature-based approach

    ELMo와 같이 task-specific 구조 사용

2) fine-tuning approach

    GPT와 같이 최소한의 task-specific parameters를 사용하고, task에 맞는 데이터로 전체 파라미터를 fine-tuning 하는 방식
    
> 두 접근 방식은 일반적인 language representations 학습을 위해 단방향 언어 모델을 사용하고, pre-training을 하는 동안 동일한 objective 함수를 공유

논문에서는 fine-tuning 접근법에서 기존 방식은 효과적인 pre-trained representations을 얻지 못하며 제한적이라고 주장함

  > 단방향 언어 모델로 인해 제한적이고, pre-training 동안 사용될 수 있는 구조의 선정을 제한함(??)
  >
  > 예를 들어, OpenAI GPT에서 left-to-right 구조를 사용하였고, self-attention layers에서 모든 토큰은 오직 이전 토큰들의 정보만을 얻을 수 있음
  >
  > 이러한 제한들은 sentence-level tasks에서 차선책이고, fine-tuning 방식들을 양방향 문맥 정보를 통합하는 것이 중요한 question answering과 같은 token-level tasks에 적용시킬 때 매우 좋지 못함
  
* * *  
  
fine-tuning 방식들을 개선한 BERT 모델 제안

Cloze task에서 영감을 얻어 "masked language model"(MLM)을 사용하여 단방향 제한을 완화시킴

  > input에서 랜덤으로 일부 토큰들을 마스킹, 문맥을 통해 기존의 단어 id를 예측하도록 학습시킴
  >
  > 기존 단방향 모델들과 달리, MLM objective는 왼쪽와 오른쪽 문맥 정보를 융합하여 representation을 가능하게 함

"next sentence prediction" task를 사용하여 텍스트 쌍 representations(두 문장 간 관계)을 함께 pre-training 시킴

### Contributions

* language representations에서 양방향 pre-training의 중요성 입증

* pre-trained representations은 특정 task를 위해 큰 구조적 변형(heavily-engineered)의 필요성을 감소시킴

  > BERT는 sentence-level, token-level tasks에서 특정 task를 위한 구조들을 뛰어넘는 SOTA 성능 달성

* 11개의 NLP tasks에서 SOTA

## Related Work

### Unsupervised Feature-based Approaches

non-neural, neural 방식들을 포함하여 수십년간 words representations에 대한 연구가 활발히 이어짐

Pre-trained word embeddings은 현대 NLP 시스템의 필수적인 요소이고, 상당한 성능 향상을 가져옴

word embedding vectors를 pre-training 하기 위해, 왼쪽과 오른쪽 문맥에서 부정확한 단어들로부터 올바르게 구별하는 objectives와 left-to-right language modeling objectives 사용

  > 이러한 방식들은 sentence embeddings이나 paragraph embeddings 처럼 거친 features를 생성함 (정밀하고 세부적인 예측이 힘듦)
  > 
  > sentence representations을 학습시키기 위해, 기존 연구에서는 이전 문장 representation이 주어지면, 다음 문장 후보를 순위 매기고 왼쪽에서 오른쪽으로 다음 문장 단어들을 생성하거나 denoising auto-encoder 사용

* * *

ELMo와 그 이전 연구들은 다른 차원을 따라 전통적인 word embedding 연구를 일반화함

left-to-right model와 right-to-left model로부터 context-sensitive features를 추출하고, 두 representation을 concat 시킴

1) 기존 task-specific 구조와 문맥적인 word embeddings을 통합할 때, ELMo는 주요 NLP benchmarks에서 SOTA 달성함

2) LSTMs을 사용하여 왼쪽, 오른쪽 문맥으로부터 단일 단어를 예측하는 문맥 representations 학습 제안

  > ELMo와 유사하지만, feature-based 방식이고 깊은 양방향 방식이 아님

3) cloze task가 텍스트 생성 모델의 성능 향상에 도움이 된다는 것을 증명

  > cloze test ? 텍스트에서 특정 단어들을 삭제하고, 그 단어가 무엇인지 맞추는 것

### Unsupervised Fine-tuning Approaches

feature-based 방식들과 마찬가지로, 먼저 unlabeled text로 pre-training 수행

최근, 문맥적인 토큰 representations을 생성하는 문장이나 문서 encoders가 unlabeled text로 pre-training 되고, supervised downstream task로 fine-tuning 됨

  > 이러한 방식은 모델을 바닥부터 학습시키기 위해(from scratch), 학습을 필요로 하는 파라미터가 적다는 장점이 있음 

  > 이런 이점을 이용하여 OpenAI GPT는 GLUE benchmark 다양한 문장 레벨 task에서 SOTA 달성 

### Transfer Learning from Supervised Data

natural language inference와 machine translation과 같이 대규모 데이터셋에서 학습되는 tasks에서 전이학습의 효과가 증명됨

또한, 컴퓨터 비전 연구에서도 전이 학습의 중요성이 입증됨

  > ImageNet으로 학습된 모델을 불러와 fine-tuning 시키는 방식이 효과적으로 사용됨

## BERT

논문의 프레임워크는 2가지 step으로 pre-training -> fine-tuning 순서로 진행됨

1. pre-training

  다양한 pre-training tasks에 대해 unlabeled data로 모델 학습
  
2. fine-tuning

  pre-trained parameters로 초기화된 BERT 모델을 특정 task를 위한 labeled data를 이용하여 fine-tuning
  
  task에 따라 독립적인 fine-tuned models을 가짐 (fig1 참고)
  
BERT는 다양한 task에 적용할 수 있는 통합된 구조이고, pre-trained 구조와 최종적인 downstream 구조 사이에 큰 차이가 없음

* * *

<img src="/assets/img/BERT/fig1.JPG" width="100%" height="100%">

* Model Architecture

BERT는 multi-layer bidirectional Transformer encoder 구조로 이루어짐

논문에서 layer의 수는 L, hidden size는 H, self-attention heads의 수는 A로 표기

BERT model은 2가지 사이즈가 있음

  > $$BERT_{BASE}$$ : L=12, H=768, A=12, Total Parameters=110M
  >
  > $$BERT_{LARGE}$$ : L=24, H=1024, A=16, Total Parameters=340M

$$BERT_{BASE}$$는 OpenAI GPT와 비교하기 위해 동일한 사이즈를 선택하였지만, BERT Transformer는 bidirectional self-attention을 사용하고 GPT Transformer는 constrained self-attention을 사용

* Input/Output Representations







 



---
title: "Bert: Pre-training of deep bidirectional transformers for language understanding review"
last_modified_at: 2022-04-16 00:00:00 -0400
categories: 
  - nlp paper
tags:
  - update
toc: true
use_math: true
toc_label: "Getting Started"
---

# Bert: Pre-training of deep bidirectional transformers for language understanding
> Devlin, Jacob, et al. "Bert: Pre-training of deep bidirectional transformers for language understanding." arXiv preprint arXiv:1810.04805 (2018).

## Abstract

새로운 language representation model인 BERT(Bidirectional Encoder Representations from Transformers) 공개

최근 language representation model과 달리, BERT는 모든 layer에서 양방향 문맥을 고려하여 pre-train 되도록 구성됨

pre-trained BERT는 task를 위한 큰 구조 변형없이 단지 하나의 추가적인 layer를 이용하여 question answering, language inference와 같은 다양한 task에서 SOTA 달성

BERT는 단순하지만 강력하게 동작함

  > 11개의 자연어처리 task에서 SOTA 달성
  >
  > GLUE score 80.5% / MultiNLI 86.7% / SQuAD v1.1 question asnwering Test F1 93.2 / SQuAD v2.0 Test F1 83.1 

## Introduction

언어 모델 pre-training은 많은 자연어처리 task에서 성능향상을 가져옴

* sentence-level task

  natural language inference, paraphrasing와 같이 전체적으로 분석하여 문장 간 관계를 예측하는 task
  
* token-level task

  named entity recognition(NER), question answering과 같이 단어 수준에서 세부적인 예측을 하는 task
  
* * *
  
* pre-trained language representations을 세부적인 task로 적용시키는 방식 

1) feature-based approach

    ELMo와 같이 task-specific 구조 사용

2) fine-tuning approach

    GPT와 같이 최소한의 task-specific parameters를 사용하고, task에 맞는 데이터로 전체 파라미터를 fine-tuning 하는 방식
    
> 두 접근 방식은 일반적인 language representations 학습을 위해 단방향 언어 모델을 사용하고, pre-training을 하는 동안 동일한 objective 함수를 공유

논문에서는 fine-tuning 접근법에서 기존 방식은 효과적인 pre-trained representations을 얻지 못하며 제한적이라고 주장함

  > 단방향 언어 모델로 인해 제한적이고, pre-training 동안 사용될 수 있는 구조의 선정을 제한함(??)
  >
  > 예를 들어, OpenAI GPT에서 left-to-right 구조를 사용하였고, self-attention layers에서 모든 토큰은 오직 이전 토큰들의 정보만을 얻을 수 있음
  >
  > 이러한 제한들은 sentence-level tasks에서 차선책이고, fine-tuning 방식들을 양방향 문맥 정보를 통합하는 것이 중요한 question answering과 같은 token-level tasks에 적용시킬 때 매우 좋지 못함
  
* * *  
  
fine-tuning 방식들을 개선한 BERT 모델 제안

Cloze task에서 영감을 얻어 "masked language model"(MLM)을 사용하여 단방향 제한을 완화시킴

  > input에서 랜덤으로 일부 토큰들을 마스킹, 문맥을 통해 기존의 단어 id를 예측하도록 학습시킴
  >
  > 기존 단방향 모델들과 달리, MLM objective는 왼쪽와 오른쪽 문맥 정보를 융합하여 representation을 가능하게 함

"next sentence prediction" task를 사용하여 텍스트 쌍 representations(두 문장 간 관계)을 함께 pre-training 시킴

### Contributions

* language representations에서 양방향 pre-training의 중요성 입증

* pre-trained representations은 특정 task를 위해 큰 구조적 변형(heavily-engineered)의 필요성을 감소시킴

  > BERT는 sentence-level, token-level tasks에서 특정 task를 위한 구조들을 뛰어넘는 SOTA 성능 달성

* 11개의 NLP tasks에서 SOTA

## Related Work

### Unsupervised Feature-based Approaches

### Unsupervised Fine-tuning Approaches

### Transfer Learning from Supervised Data

## BERT






 



---
title: "[Paper Review] Exploring the limits of transfer learning with a unified text-to-text transformer"
last_modified_at: 2022-08-03 00:00:00 -0400
categories: 
  - nlp paper
tags:
  - update
toc: true
use_math: true
toc_label: "Getting Started"
---

# Exploring the limits of transfer learning with a unified text-to-text transformer
> Raffel, Colin, et al. "Exploring the limits of transfer learning with a unified text-to-text transformer." J. Mach. Learn. Res. 21.140 (2020): 1-67.

# Abstract

자연어 처리에서 데이터가 풍부한 task로 모델을 pretraining 하고 downstream task로 fine-tuning 하는 Transfer learning 기법이 널리 사용되었고, 
다양한 접근 방식, 방법론이 등장하였습니다.

본 논문에서는 모든 text 기반 언어 문제를 text-to-text 형태로 변환한 통합된 프레임워크를 소개하고, NLP의 다양한 transfer learning 기법들을 탐구하였습니다.

> 수십 가지 자연어 이해 task에 대한 pretraining objectives, 모델 구조, unlabeled data sets, transfer 방식, 이외에도 다양한 요소들을 비교

실험을 통해 얻은 통찰력과 모델 scaling, 새로운 데이터셋 "Colossal Clean Crawled Corpus"를 결합하여 요약, QA, 텍스트 분류 등 많은 벤치마크에서 SOTA 성능을 달성하였습니다.

논문 저자들은 NLP의 transfer learning 차후 연구들이 활용할 수 있도록 데이터셋과 pre-trained models, 코드를 공개하였습니다.

# 1. Introduction

<!-- 자연어 처리 tasks를 수행하기 위한 머신러닝 모델이 downstream learning을 처리할 수 있도록 학습되기를 원하고, 이는 모델이 텍스트를 이해하는 범용적인 지식을 학습한다고 볼 수 있습니다.

이러한 지식은 단어의 철자나 의미와 같은 low-level 부터 문장 전체를 이해해야 하는 high-level 까지 다양한 범위를 필요로 합니다.

현대 머신러닝에서 이러한 지식을 제공하는 것이 명시적으로 수행되는 경우는 드물지만, 보조 task로서 학습됩니다.

예를 들어, word vectors를 이용하기 위해 단어를 continuous representation으로 맵핑해주는 방식을 사용하고, 여기서 비슷한 단어들이 비슷한 위치의 vectors로 맵핑되도록 학습됩니다.

* * * -->

최근에는 데이터가 풍부한 task로 전체 모델을 사전 학습시키는 방식이 많이 사용되고 있고, 
사전 학습은 모델이 downstream tasks로 전이되기 위한 범용적인 능력과 지식을 발전시킬 수 있도록 합니다.

- 컴퓨터 비전 분야의 사전 학습

  라벨이 있는 대규모 데이터셋(ImageNet)을 활용하여 **supervised learning** 방식으로 pretraining 수행
  
- 자연어 처리 분야의 사전 학습

  라벨이 없는 데이터를 활용하여 **unsupervised learning** 방식으로 pretraining 수행
  
NLP의 unsupervised pre-training은 인터넷에서 대규모 텍스트를 얻을 수 있기 때문에 특히 매력적인 방식입니다.

> **the Common Crawl project** : 매달 웹 페이지를 통해 추출된 20TB 텍스트를 생성

뉴럴 네트워크는 큰 데이터셋에서 큰 모델을 학습시킬수록 성능을 좋아지는 특성을 갖고 있기 때문에 대규모 데이터를 활용할 수 있다는 것은 큰 이점이 될 수 있습니다.

이러한 효과로 NLP에 다양한 전이 학습 방법론이 발전하면서 방대한 양의 연구들이 등장하였고, 이는 다양한 pre-training objectives, unlabeled datasets, benchmarks, fine-tuning 방법 등을 만들어냈습니다.

해당 분야가 급속도로 성장하면서 다른 알고리즘을 비교하고, 새로운 기여의 효과를 분석하고, 전이 학습을 위한 기존 방법들을 이해하는 것이 어려울 수 있습니다.

논문에서는 더욱 엄밀한 이해를 위해 전이 학습을 위한 통합된 접근법을 활용하였고, 이를 통해 다양한 접근법을 체계적으로 연구하고 현재 접근법의 한계점을 실험하였습니다.

* * *

본 논문의 근원적인 아이디어는 모든 텍스트 처리 문제를 텍스트를 입력으로 받고 새로운 텍스트를 출력하는 **text-to-text** 문제로 다루는 것입니다.

이러한 접근법은 NLP tasks에 기존 통합 프레임워크들에서 영감을 얻었습니다.

> 모든 텍스트 문제를 question answering 문제 or language modeling 문제 or span extraction 문제로 해결한 기존 연구들에서 영감을 얻음

결정적으로, text-to-text 프레임워크를 사용하면 논문에서 고려한 모든 task에 대해 모델, objective, 학습 절차, 디코딩 과정을 동일하게 적용할 수 있습니다.

> question answering, document summarization, sentiment classification, to name a few 등을 포함하는 다양한 영어 기반 NLP tasks에서 성능을 평가

<img src="/assets/img/T5/fig1.JPG" width="100%" height="100%">

통합된 프레임워크를 사용하여 다양한 전이 학습 objectives, unlabeled datasets과 다른 요소들의 효과를 비교하고, 모델과 데이터셋의 규모를 조정하면서 NLP 전이 학습의 한계를 탐구할 수 있었습니다.

논문의 목표는 새로운 방식을 제안하는 것이 아니고, 해당 분야에 대한 포괄적인 관점을 제공하는 것이며 기존 기법들을 조사하고 탐구하여 실험을 통해 비교하는 내용으로 논문이 구성되어 있습니다.

- scaling up

  기존 접근법들의 한계를 알아보기 위해 다양한 모델 사이즈를 실험했고, 110억개의 파라미터를 갖는 모델로 많은 task에서 SOTA 성능을 달성하였습니다.

- Colossal Clean Crawled Corpus (C4)

  웹에서 추출된 수백 기가바이트의 정제된 영어 텍스트로 구성된 새로운 데이터셋을 소개하였습니다.
  

  









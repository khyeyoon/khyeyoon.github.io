---
title: "[Paper Review] Deep contextualized word representations"
last_modified_at: 2022-07-26 00:00:00 -0400
categories: 
  - nlp paper
tags:
  - update
toc: true
use_math: true
toc_label: "Getting Started"
---

# Deep contextualized word representations
> Matthew E. Peters, Mark Neumann, et al. "Deep contextualized word representations" NAACL 2018.

## Abstract

본 논문에서는 새로운 유형의 deep contextualized word representation을 제안

> (1) 합성어, 의미론과 같이 단어 사용의 복잡한 특성을 모델링

> (2) 다의어를 적절히 다루기 위해, 언어적 맥락에 따라 단어가 어떻게 사용되는지를 모델링

대규모 text corpus로 사전 학습된 deep bidirectional language model (biLM)의 내부적인 states를 활용하여 word vectors 학습

이러한 representations은 기존 모델에 쉽게 추가할 수 있고, 다양한 NLP task(question answering, textual entailment, sentiment analysis 등)에서 SOTA 성능을 향상시킴

또한 사전 훈련된 네트워크의 깊은 내부를 노출하는 것이 중요하다는 분석을 제시하고, downstream models이 다양한 유형의 semi-supervision signals를 혼합할 수 있도록 함

## Introduction

neural language understanding models에서 사전 학습된 word representations은 핵심적인 요소이지만, 질 좋은 representations을 학습하는 것은 어려움

- word representation을 위한 모델의 이상적인 2가지 능력

  1. 구문과 의미론과 같이 단어 사용의 복잡한 특성 모델링

  2. 다의어를 구분할 수 있도록 문맥에 따라 어떻게 단어가 달라지는지 모델링

본 논문에서는 이러한 두가지 challenges를 직접 해결한 새로운 유형의 deep contextualized word representation을 소개

> 2가지 모델링 능력 + 기존 모델과 쉽게 통합 가능 + 다양한 NLU task에서 SOTA 향상

* * *

각 토큰을 representation 할 때, 전체 입력 문장을 보고 표현된다는 점에서 전통적인 word embedding 방식과 다름

대규모 text corpus에 대해 LM objective와 결합되어 학습된 양방향 LSTM으로부터 나온 벡터를 사용함

이러한 이유로 제안한 representation을 ELMo(Embeddings from Language Models) representations이라고 부름

기존 워드 벡터 학습 방식과 달리, ELMo representations은 biLM의 내부 모든 layers를 활용하기 때문에 깊음

구체적으로, 본 논문에서는 task를 위한 각각의 입력 단어 위에 쌓인 벡터의 선형 결합을 학습하고, 이는 가장 상위 LSTM layer만 사용하는 것에 비해 매우 향상된 성능을 보였음

내부적인 states를 결합하는 것은 매우 풍부한 word representations을 만들어줌

higher-level LSTM states는 단어 의미릐 문맥 의존적인 면을 잘 파악하고, lower-level states는 문법적인 면을 잘 파악함

> 이러한 모든 신호들을 동시에 활용하는 것은 매우 유익하고, 학습된 모델이 최종적인 task에 가장 유용한 semi-supervision을 선택할 수 있도록 함

- ELMo 실험 결과

  ELMo representations은 실제로 잘 동작하는 것을 확인할 수 있었음
  
  - ELMo는 6개의 language understanding 문제를 위한 기존 모델들에 쉽게 추가될 수 있음

    ex) textual entailment, question answering, sentiment analysis 등
      
  - ELMo representations의 추가는 SOTA를 상당히 향상시킴

    직접적인 비교가 가능한 task에서 ELMo는 CoVe의 성능을 뛰어넘음
    
  - ELMo와 CoVe를 분석하는 과정에서 deep representations이 LSTM의 상위 layers만을 이용하는 것을 뛰어넘는다는 사실을 알아냄

  - 또한, ELMo가 더 다양한 NLP task에서도 좋은 성능을 낼 것이라고 기대하고 있음

## Related word


## ELMo: Embeddings from Language Models
    

<!-- <img src="/assets/img/STAR-Net/fig2.JPG" width="90%" height="90%"> -->









